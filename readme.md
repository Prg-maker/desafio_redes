
# ğŸ” AnÃ¡lise ExploratÃ³ria

Nosso conjunto de dados possui trÃªs classes: **Galaxy**, **Star** e **QSO**.

O objetivo inicial Ã© **identificar padrÃµes e compreender o comportamento dessas classes**.

As variÃ¡veis selecionadas para essa anÃ¡lise foram: **`u`**, **`g`**, **`r`**, **`z`** e **`redshift`**.

## ğŸ§  Primeira HipÃ³tese

Quando o valor de **`redshift`** Ã© **alto**, existe uma maior probabilidade de o objeto ser uma **QSO (Quasar)** ou uma **galÃ¡xia**.

Por outro lado, quando o valor de **`redshift`** Ã© **menor que 1**, hÃ¡ maior probabilidade de o objeto ser uma **estrela**, pois objetos estelares estÃ£o mais prÃ³ximos da Terra e, portanto, apresentam um **redshift** mais baixo.

---

# ğŸ”— Modelagem com Redes Neurais

## ğŸ”¸ Primeira Rede Neural

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Camada                         â”‚ SaÃ­da (Shape)         â”‚ ParÃ¢metros     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Dense (dense_1)                â”‚ (None, 16)            â”‚ 192            â”‚
â”‚ Dense (dense_2)                â”‚ (None, 16)            â”‚ 272            â”‚
â”‚ Dense (dense_3)                â”‚ (None, 3)             â”‚ 51             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
- **AcurÃ¡cia:** 0.96845

![DescriÃ§Ã£o da imagem](primeira_rede.png) <br>
*Figura 1 - ComparaÃ§Ã£o de previsÃ£o e valor real (do primeiro modelo).*
---
#### ğŸ”¸ Na primeira tentativa, utilizei a funÃ§Ã£o de ativaÃ§Ã£o ReLU, com 16 neurÃ´nios na primeira e na segunda camada, e 3 neurÃ´nios na camada de saÃ­da, correspondentes Ã s trÃªs classes do problema. Essa configuraÃ§Ã£o resultou em uma acurÃ¡cia de 0.96845. 

#### ğŸ”¸ Em seguida, decidi testar diferentes arquiteturas, sem me preocupar, inicialmente, com ajustes mais finos como backpropagation, regularizaÃ§Ã£o ou otimizaÃ§Ã£o dos hiperparÃ¢metros.

#### ğŸ”¸ Segunda Rede Neural ComparaÃ§Ã£o de previsÃ£o e valor real (do primeiro modelo), que o valor real vs o valor previsto, em outros modelos nao vai ter esse grafico por que nao chega uma coisa diferente para todos.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Camada                         â”‚ SaÃ­da (Shape)         â”‚ ParÃ¢metros     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Dense (dense_4)                â”‚ (None, 16)            â”‚ 192            â”‚
â”‚ Dense (dense_5)                â”‚ (None, 8)             â”‚ 136            â”‚
â”‚ Dense (dense_6)                â”‚ (None, 6)             â”‚ 54             â”‚
â”‚ Dense (dense_7)                â”‚ (None, 6)             â”‚ 42             â”‚
â”‚ Dense (dense_8)                â”‚ (None, 3)             â”‚ 21             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
- **AcurÃ¡cia:** 0.96365


![DescriÃ§Ã£o da imagem](segunda.png)
![DescriÃ§Ã£o da imagem](segunda2.png)
---
#### ğŸ”¸ Na segunda rede, aumentei o nÃºmero de camadas e distribuÃ­ os neurÃ´nios entre elas, com o objetivo de verificar se isso aumentaria o nÃ­vel de acurÃ¡cia. AlÃ©m disso, passei a observar tambÃ©m o comportamento da funÃ§Ã£o de perda (loss function).

#### ğŸ”¸ O modelo atingiu uma acurÃ¡cia de 0.96365, porÃ©m, ao plotar o grÃ¡fico de perda, percebi que o loss nÃ£o estava convergindo para um valor estÃ¡vel, indicando possÃ­vel dificuldade no ajuste dos pesos ou problemas relacionados Ã  arquitetura escolhida.


## ğŸ”¸ Terceira Rede Neural

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Camada                         â”‚ SaÃ­da (Shape)         â”‚ ParÃ¢metros     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Dense (dense_19)               â”‚ (None, 16)            â”‚ 192            â”‚
â”‚ Dense (dense_20)               â”‚ (None, 8)             â”‚ 136            â”‚
â”‚ Dense (dense_21)               â”‚ (None, 8)             â”‚ 72             â”‚
â”‚ Dense (dense_22)               â”‚ (None, 3)             â”‚ 27             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
- **AcurÃ¡cia:** 0.97205

![DescriÃ§Ã£o da imagem](terceira-1.png)
![DescriÃ§Ã£o da imagem](terceira-2.png)
![DescriÃ§Ã£o da imagem](terceiraBac.png)

#### ğŸ”¸ Na terceira rede treinada, utilizei uma arquitetura com 16 neurÃ´nios na primeira camada, seguido de duas camadas com 8 neurÃ´nios cada, e uma camada de saÃ­da com 3 neurÃ´nios. Observei um aumento na acurÃ¡cia, que chegou a 0.97205.
#### ğŸ”¸ ApÃ³s plotar o grÃ¡fico da funÃ§Ã£o de perda, percebi que o loss estava convergindo corretamente para um valor estÃ¡vel. Realizei tambÃ©m a anÃ¡lise do backpropagation e confirmei que o modelo estava de fato convergindo.
#### ğŸ”¸ Testei algumas variaÃ§Ãµes, como o aumento do nÃºmero de camadas, mas essas mudanÃ§as nÃ£o impactaram significativamente a acurÃ¡cia final.
---
#### Conclusao:  Vi que o terceiro modelo se saiu melhor que todos, explorei outros nives de camadas, vi que os numeros de camdas nao siginifcas um bom resultado e nem se aumentar os niveis de neuronios aumenta a qualidade do modelo. 
